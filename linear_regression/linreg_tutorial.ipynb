{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Two ways\n",
    "\n",
    "1. The Analytical Solution  \n",
    "2. By Gradient Descent\n",
    "\n",
    "#### Prerequisites: Calculus, Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import depedencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we make a toy dataset by creating a function that:**\n",
    "1. Randomly initializes a line \n",
    "2. Randomly samples points in the domain\n",
    "3. Returns an array of the points on the line with added gaussian noise\n",
    "\n",
    "**Additionally, we make a visualization function that plots the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(size, mean=10.0, std=5.0):\n",
    "    \"\"\"Returns a toy linear dataset (x,y)\n",
    "    \"\"\"\n",
    "    # randomly initialize a line\n",
    "    \n",
    "    \n",
    "    # Generate arrays to be sampled from\n",
    "    \n",
    "    \n",
    "    # Add noise to the y output\n",
    "    \n",
    "    \n",
    "    return x, sampled_y\n",
    "\n",
    "def show_dataset(x, y, lines=[]):\n",
    "    \"\"\"Plots a dataset and an arbitrary number of input lines\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, c='r')\n",
    "    plt.xlim(min(x), max(x))\n",
    "    for line in lines:\n",
    "        plt.plot(x, line[0], label=line[1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata, ydata = generate_data(100, std=10)\n",
    "lines = [(0.5*x+10, 'random line')]\n",
    "show_dataset(xdata, ydata, lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\x}{X}$\n",
    "$\\newcommand{\\L}{\\mathcal{L}}$\n",
    "$\\newcommand{\\t}{\\theta}$\n",
    "$\\newcommand{\\yh}{\\hat{\\textbf{y}}}$\n",
    "$\\newcommand{\\y}{\\textbf{y}}$\n",
    "## Method 1: Analytical Solution\n",
    "* Equation of a line  <br>\n",
    "$$ \\hat{y} = mx + b $$  <br>\n",
    "         \n",
    "* This method computes the optimal parameters for the linear model in one-shot. <br>\n",
    "    1. In matrix-vector form, the linear equation is:  <br><br>\n",
    "$$\n",
    "\\yh = \\x\\t\\qquad\\t\\in{\\mathbb{R}^{n+1},\\quad\\x\\in{\\mathbb{R}^{m\\times(n+1)}}}\n",
    "$$<br>\n",
    "Where $\\x$ is a matrix of features with 1 appended to the end and $\\t$ is a vector of parameters with the bias appended to the end. <br><br>\n",
    "        \n",
    "    2. Mean-squared-error Loss function - This is desirable since MSE is convex i.e. it has a global minimum. <br>  \n",
    "        $$ L = \\frac{1}{2}\\sum_{i=1}^{N} (y_i-\\hat{y}_i)^2 = \\frac{1}{2} \\| \\textbf{y} - \\hat{\\textbf{y}} \\|^{2}_2\n",
    "             = \\frac{1}{2}\\| \\y - \\x\\t\\|^{2}_2 = \\frac{1}{2}(\\y-\\x\\t)^\\top (\\y - \\x\\t)$$  <br><br>\n",
    "* Now we can can go about deriving the analytical solution by solving the following optimization program: <br><br>\n",
    "$$ \\min_\\t \\L(\\x,\\y;\\t) $$ <br>\n",
    "This is called Empirical Risk Minimization (ERM).\n",
    "\n",
    "    1. The first step is to expand the loss function and leave out the constant factor in front for convenience:<br>\n",
    "    \n",
    "        $$ \\L = \\frac{1}{2}((\\y^\\top - (\\x\\t)^\\top)(\\y - \\x\\t)$$ <br>\n",
    "        $$ \\hspace{0.4cm} = \\frac{1}{2}(\\y^\\top \\y-\\y^\\top (\\x\\t) -(\\x\\t)^\\top \\y+ (\\x\\t)^\\top \\x\\t)$$ <br>\n",
    "        $$ \\hspace{0.4cm} = \\frac{1}{2}(\\y^\\top \\y-\\y^\\top (\\x\\t) -\\t^\\top \\x^\\top \\y+ \\t^\\top\\x^\\top \\x\\t)$$ <br>\n",
    "        $$ \\hspace{0.4cm} = \\frac{1}{2}(\\t^\\top \\x^\\top\\x\\t-2\\y^\\top \\x\\t+\\y^\\top \\y )$$ <br>\n",
    "        \n",
    "    2. Now we use some results of matrix calculus <br><br>\n",
    "        $$\\frac{\\partial{(\\x\\t)}}{\\partial{\\t}} = \\x^T, \\quad\\frac{\\partial({\\t^\\top M\\t)}}{\\partial{\\t}}=M\\t+M^\\top \\t$$ <br>\n",
    "        <br>\n",
    "    3. We can now finish the derivation for the optimal $\\t$ by computing $\\frac{\\partial{\\L}}{\\partial{\\t}}$ and setting it equal to zero: <br>\n",
    "        \n",
    "        $$\\frac{\\partial{\\L}}{\\partial{\\t}}\\Bigr\\rvert_{\\t=\\t} = \\frac{1}{2}\\x^\\top \\x\\t+\\frac{1}{2}(\\x^\\top \\x)^\\top \\t-\\x^\\top \\y= \\x^\\top (\\x\\t - \\y)$$<br>\n",
    "        $$\\x^\\top \\x\\t^* -\\x^\\top \\y = 0$$<br>\n",
    "        $$\\x^\\top \\x\\t^{*}=\\x^\\top \\y$$<br>\n",
    "        $$\\t^{*}=(\\x^\\top \\x)^{-1}\\x^\\top \\y$$ <br><br>\n",
    "        \n",
    "    4. The expression $A^+ = (\\x^\\top \\x)^{-1}\\x^\\top$ from above is also known as the [Moore-Penrose Psuedoinverse](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.9-The-Moore-Penrose-Pseudoinverse/). It is used to approximately solve linear equations of the form $Ax=b$ with $AA^+\\approx I$ (in the case that $A^{-1}$ may not exist)\n",
    "\n",
    "    \n",
    "* Finally, lets write the code to implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape a copy of x into a matrix with a 1's column appended to it\n",
    "X = \n",
    "print(X[20:30])  # Show a few entries to get the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally just apply the equation using numpy! \n",
    "# Note: The output will be a [2,1] vector containing m and b\n",
    "params = \n",
    "\n",
    "show_dataset(xdata, ydata, [(X@params, \"Analytical\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Gradient Descent\n",
    "\n",
    "* First lets define a few things: <br>\n",
    "    1. Equation of a line  <br><br>\n",
    "        - Standard form: $ \\hat{y} = mx + b $  <br><br>\n",
    "        - Matrix-vector form: $\\hat{\\textbf{y}} = X \\theta$ <br><br>\n",
    "         \n",
    "    2. Mean-squared-error Loss function - This is desirable since MSE is convex i.e. it has a global minimum. <br>  \n",
    "        $$ L = \\frac{1}{2}\\sum_{i=1}^{N} (y_i-\\hat{y}_i)^2 = \\frac{1}{2} \\| \\textbf{y} - \\hat{\\textbf{y}} \\|^{2}_2$$  <br><br>\n",
    "          \n",
    "    3. Gradient Descent Rule - This is how the parameters get updated such that the loss is minimized.  <br><br>\n",
    "        $$ \\theta := \\theta - \\alpha \\nabla_\\theta \\mathcal{L} $$ <br>\n",
    "\n",
    "* Here is a picture of what gradient descent is doing:\n",
    "\n",
    "![alt text](https://saugatbhattarai.com.np/wp-content/uploads/2018/06/gradient-descent-1.jpg \"Logo Title Text 1\")\n",
    "\n",
    "* All we need to do is use the gradient w.r.t. the parameters we calculated from Method 1:  <br><br> \n",
    "$$\\nabla_\\theta \\mathcal{L} = \\x^\\top (\\x\\t - \\y)$$ <br><br>\n",
    "\n",
    "* Now we have the tools to perform linear regression by gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseloss(value, target):\n",
    "    \"\"\"Computes the mean squared error (MSE) loss function\n",
    "    \"\"\"\n",
    "    return 0.5 * np.linalg.norm(target - value)**2\n",
    "\n",
    "def gradient_descent(xs, ys, epochs, lr, num_lines=4):\n",
    "    \"\"\"Implements gradient descent on a dataset to reduce the MSE loss\n",
    "    \"\"\"\n",
    "    # Initialize the parameters\n",
    "    \n",
    "    \n",
    "    # Unpack the dataset and begin fitting\n",
    "    lines = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        gradient = \n",
    "        theta -= \n",
    "            \n",
    "        # Computed the updated predictions\n",
    "        predictions = \n",
    "            \n",
    "        # Add lines to plot and print the loss\n",
    "        if epoch % int(epochs / num_lines) == 0:\n",
    "            lines.append((predictions, f'epoch {epoch}'))\n",
    "            print(f\"Epoch: {epoch:4d} MSE Loss: {mseloss(predictions, ys):.4f}\")\n",
    "        \n",
    "    #Show the fitted lines\n",
    "    show_dataset(xs[:, 0], ys, lines)\n",
    "    \n",
    "    return xs @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the loss compared to the gradient descent method\n",
    "y_hat = \n",
    "analytical_loss = \n",
    "grad_descent_loss = \n",
    "print(\"Analytical solution loss: {0:.4f}\\nGradient descent loss: {1:.4f}\".format(\n",
    "    analytical_loss, grad_descent_loss\n",
    "))\n",
    "\n",
    "lines = [\n",
    "    (y_hat, \"Analytical\"),\n",
    "    (y_pred, \"Gradient Descent\")\n",
    "]\n",
    "show_dataset(xdata, ydata, lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after 500 iterations, the analytical solution beats the gradient descent solution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
