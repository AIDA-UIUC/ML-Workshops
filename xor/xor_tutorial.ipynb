{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem: XOR Classification\n",
    "- Consider the logical operation XOR (Exclusive-or):\n",
    "\n",
    "| bits | output |\n",
    "|-|-|\n",
    "| 1   0  | 1 |\n",
    "| 0   1  | 1 |\n",
    "| 0   0  | 0 |\n",
    "| 1   1  | 0 |\n",
    "\n",
    "- Can a linear model fit this function? Let's look at the graphical representation of the XOR function:\n",
    "<img src=\"images/xor.png\" />\n",
    "\n",
    "- Clearly the XOR is not **linearly separable**, it is **nonlinear**\n",
    "- This kind of problem is perfect for a **neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "- How can we model neurons in the brain and computationally represent them?\n",
    "<img src=\"images/neuron.png\" />\n",
    "\n",
    "- From this basic building block we can begin to build simple neural networks. The XOR neural network looks like:\n",
    "\n",
    "<img src=\"images/xornn.png\" />\n",
    "\n",
    "- For now, we use this graph representation of neural networks for ease of visualization, but it does not scale to larger networks. Remember that neural networks are really just functions that look like this:\n",
    "\n",
    "$$ NN(x) = f(W_nf(W_{n-1}f(...f(W_1x + b_1)) + b_{n-1}) + b_n) $$\n",
    "\n",
    "- Note that $ f(\\cdot) $ **must** be nonlinear in order to model nonlinear functions, otherwise it's easy to show that if $ f(\\cdot) $ is linear, then the entire neural network reduces to a linear model and were back to square one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(43252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generating function\n",
    "def generate_data():\n",
    "    \"\"\"Generates bitpairs for the xor neural network\n",
    "    \"\"\"\n",
    "    bitpairs = np.array([[1,0],\n",
    "                         [0,1],\n",
    "                         [0,0],\n",
    "                         [1,1]])  # data\n",
    "    xor_labels = np.array([[1],\n",
    "                           [1],\n",
    "                           [0],\n",
    "                           [0]])  # labels\n",
    "    return bitpairs, xor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initalization scheme for the network\n",
    "def get_parameters():\n",
    "    \"\"\"Generates weights for the XOR network sampled from the normal distribution\n",
    "    \"\"\"\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging tool for checking tensor shapes\n",
    "def print_shapes(ls, name):\n",
    "    \"\"\"Helper to print shapes of elements in a list\n",
    "    \"\"\"\n",
    "    print(f\"{name}: {[l.shape for l in ls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "- The sigmoid functions maps any real input to a probability value $ \\sigma \\in [0, 1] $  \n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$  \n",
    "  \n",
    "- We use the sigmoid function for the XOR neural network because our goal is to predict values in the set $ \\{0, 1\\} $, and the sigmoid function allows us to output a continuous approximation of this discrete set  \n",
    "- Techincally we can use different activation functions for every layer (as long as the output is in $ [0, 1] $ ), but for the purposes of this tutorial, we will use the sigmoid function throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function\n",
    "def sigmoid(x, deriv=False):\n",
    "    \"\"\"Defines the sigmoid activation function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy Function\n",
    "- The cross entropy function is used as a distance measure between probabilty distributions\n",
    "- The discrete version of cross entropy is as follows:\n",
    "\n",
    "$$ H(\\textbf{p}, \\textbf{q}) = -\\sum_{i = 1}^m q_i\\log(p_i) $$\n",
    "\n",
    "- Since we're predicting a single probability value, we can think of our discrete output distribution as $ [\\sigma, 1 - \\sigma] $, where $\\sigma$ is the output from the neural network, and the discrete target distribution as $ [y, 1 - y] $ and thus, $ m = 2 $. So the cross entropy function reduces to the **binary cross entropy** function:  \n",
    "\n",
    "$$ H(p, q) = -(q\\log(p) + (1 - q)\\log(1 - p)) $$\n",
    "\n",
    "- Where p and q are the predicted and target distributions respectively. Our optimization objective is to minimize the binary cross entropy between the XOR labels and the network outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def binary_xentropy(target, p, deriv=False):\n",
    "    \"\"\"Defines the binary cross entropy loss function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    return -(target * np.log(p) + (1 - target) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "- In SGD (stochastic gradient descent), we make updates on the parameters over randomly shuffled **mini-batches**.  \n",
    "  \n",
    "  \n",
    "- Why do we shuffle and batch, rather than computing gradients over the entire dataset once?  \n",
    "    + For big datasets, this could be computationally intractable  \n",
    "    + For non-convex functions or functions with many local minima, gradient descent could get stuck a bad minima  \n",
    "  \n",
    "  \n",
    "- The stochasticity introduced by the random shuffling then batching in SGD allows our optimization procedure to make   noisy gradient updates, and possibly 'jump out' of bad local minima we land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_batch(X, Y, batch_size):\n",
    "    \"\"\"Method to shuffle and batch training data\n",
    "    \"\"\"\n",
    "    assert batch_size <= len(X), print(\"Batch size must be smaller than the dataset size\")\n",
    "    \n",
    "    # Return a generator for the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forward Pass: How neural networks process data\n",
    "- Neural networks process data through forward propogation i.e. sending the data input signal through all neurons\n",
    "- The math for the forward propogation is simple:  \n",
    "  \n",
    "$$ z_j^{l} = \\sum_{i = 1}^{n} a_i^{l - 1}w_{ij}^{l} + b_j^{l} $$  \n",
    "  \n",
    "   \n",
    "$$ a_j^{l} = \\sigma(z_j^l) $$\n",
    "\n",
    "- In matrix-vector notation:\n",
    "\n",
    "$$ \\textbf{z}^{l} = \\textbf{a}^{l - 1}\\textbf{W}^l + \\textbf{b}^l $$  \n",
    "\n",
    "  \n",
    "$$ \\textbf{a}^{l} = \\sigma(\\textbf{z}^{l})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(x, weights, biases):\n",
    "    \"\"\"Perform feed forward operation\n",
    "    \"\"\"\n",
    "    \n",
    "    return zs, activations, d_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Backward Pass: The workhorse of neural network optimization\n",
    "- At this point, we know how to use gradient descent to optimize parameters for a 'single' layer linear model, but how do we optimize a multi-layer neural network e.g. $ f(x) = \\sigma(w_2\\sigma(w_1x + b_1) + b_2) $ ?\n",
    "- Thanks to Geoffrey Hinton, in 1986 he invented [an algorithm](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) to solve this problem called back-propogation\n",
    "- Michael Nielsen has made an amazing modern tutorial on backpropogation [here](http://neuralnetworksanddeeplearning.com/chap2.html)  \n",
    "  \n",
    "  \n",
    "- Our optimization objective is to find $ \\nabla_\\theta\\mathcal{L} $ so that we can adjust the parameters $\\theta$ in the direction that minimizes our loss function $\\mathcal{L}$\n",
    "- From Michael Nielsen's website there are four fundamental equations to backpropogation (proofs are provided on his website, two are left as an exercise)\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{L} = \\nabla_a{\\mathcal{L}}\\odot\\sigma^\\prime(z^L)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{l} = (\\delta^{l + 1}(w^{l + 1})^T)\\odot\\sigma^\\prime(z^l)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{w_{jk}^l}} = (a^{l-1})^T \\delta_j^l\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{b_{j}^l}} = \\delta_j^l\n",
    "\\end{equation}\n",
    "\n",
    "- ### TODO: include proofs, for now go over Michael Nielsen's tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y, weights, biases):\n",
    "    \"\"\"Obtain gradients through backprop\n",
    "    \"\"\"\n",
    "        \n",
    "    return gradw, gradb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD: A recap\n",
    "- We've seen how to do gradient descent applied to linear regression for one-dimensional inputs, its a minor adjustment to handle multiple dimensional gradient updates:\n",
    "\n",
    "\\begin{equation}\n",
    "W^l := W^l - \\alpha\\nabla_{W^l}\\mathcal{L}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b^l := b^l - \\alpha\\nabla_{b^l}\\mathcal{L}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, Y, batch_size, lr, weights, biases):\n",
    "    \"\"\"Performs stochastic gradient descent over mini batches\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, weights, biases, epochs=100, lr=1e-3, batch_size=4):\n",
    "    \"\"\"Trains the XOR neural network with SGD\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # perform stochastic gradient through the network\n",
    "\n",
    "\n",
    "        # evaluate network\n",
    "\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            print(f\"Running epoch {epoch:06d} - loss: {epoch_loss:.06f}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, weights, biases):\n",
    "    \"\"\"Make predictions on the dataset with the trained network\n",
    "    \"\"\"\n",
    "    X, _ = dataset\n",
    "    _, activations, _ = feedforward(X, weights, biases)\n",
    "    print(\"Truth table predictions\")\n",
    "    print(\"_______________________\")\n",
    "    for x, pred in zip(X, activations[-1]):\n",
    "        print(f\"{x[0]} {x[1]} | {pred[0]:.14f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    \"\"\"Plots the losses of the network over the training phase\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(f\"Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"binary cross-entropy loss\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters and get the data\n",
    "\n",
    "\n",
    "# train the network and plot the loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the surface of activations that the network learned\n",
    "def plot_activation_surface(weights, biases, three_dim=False):\n",
    "\n",
    "    X, Y = np.mgrid[0:1.1:0.1, 0:1.1:0.1]\n",
    "    dataset = np.array([[x, y] for rowx, rowy in zip(Y, X) for x, y in zip(rowx, rowy)])\n",
    "    \n",
    "    _, activations, _ = feedforward(dataset, weights, biases)\n",
    "    activation_grid = activations[-1].reshape(X.shape[0], Y.shape[0]).T\n",
    "    \n",
    "    if three_dim:\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.view_init(50, 120)\n",
    "        ax.plot_surface(X, Y, activation_grid, cmap='binary_r')\n",
    "    else:\n",
    "        plt.xlim(0, 10)\n",
    "        plt.ylim(0, 10)\n",
    "        plt.imshow(activation_grid, cmap='binary_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2d activation map\n",
    "plot_activation_surface(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3d activation map\n",
    "plot_activation_surface(weights, biases, three_dim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
