{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Disclaimer: Material is credited to\n",
    "+ [Stanford 224N's](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf) lecture on word embeddings\n",
    "\n",
    "\n",
    "+ [Pat Coady's](https://github.com/pat-coady/word2vec) repo\n",
    "\n",
    "\n",
    "\n",
    "+ [This](https://gist.github.com/aneesh-joshi/c8a451502958fa367d84bf038081ee4b) amazing github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representations\n",
    "\n",
    "## How can we turn words into objects that a computer can process?\n",
    "\n",
    "## Words as discrete representations:\n",
    "+ In traditional NLP, words are often represented as unique one-hot encoded vectors, e.g. cat = [0, 0, 0, 1], dog = [0, 0, 1, 0]\n",
    "+ The length of these vectors is determined by the vocabulary size of the training data, i.e. how many unqiue words there are\n",
    "+ The problems with this approach?\n",
    "    - A lot of memory required to represent a large corpus, Need [V x V] matrix where V is the vocabulary size\n",
    "    - These word vectors are sparse, most of the elements in the word vector matrix are 0\n",
    "    - There is no natural measure of similarity between these word vectors,\n",
    "        i.e. hotel = [0, 0, 1], motel = [0, 1, 0] are orthogonal vectors with similar meaning\n",
    "    - **We want to encode similarity within the word vectors**\n",
    "        \n",
    "## Words as continuous representations\n",
    "+ Distributional semantics is the notion that a word's meaning is given by the words that appear close by\n",
    "+ “You shall know a word by the company it keeps” - J. R. Firth 1957\n",
    "+ One of the most successful ideas of modern statistical NLP!\n",
    "+ We will use dense representations of words so that it is similar to words that often appear in the same context\n",
    "+ Example: cat = [0.3, 1.5, -0.4, 1.9]\n",
    "+ With continuous representations, we can naturally measure similarity by using the dot product in the vector space formed by our vectors!\n",
    "+ The length of the vectors in this case is a hyperparameter, so we only need a [V x D] matrix with D < V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jefflai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataloader\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "tf.random.set_random_seed(1021)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-21 16:59:43--  https://www.gutenberg.org/files/11/11-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173595 (170K) [text/plain]\n",
      "Saving to: ‘alice-in-wonderland.txt’\n",
      "\n",
      "alice-in-wonderland 100%[===================>] 169.53K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-10-21 16:59:43 (1.14 MB/s) - ‘alice-in-wonderland.txt’ saved [173595/173595]\n",
      "\n",
      "--2019-10-21 16:59:43--  https://www.gutenberg.org/files/1661/1661-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607788 (594K) [text/plain]\n",
      "Saving to: ‘sherlock-holmes.txt’\n",
      "\n",
      "sherlock-holmes.txt 100%[===================>] 593.54K  2.22MB/s    in 0.3s    \n",
      "\n",
      "2019-10-21 16:59:44 (2.22 MB/s) - ‘sherlock-holmes.txt’ saved [607788/607788]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download our data\n",
    "!wget https://www.gutenberg.org/files/11/11-0.txt -O alice-in-wonderland.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "+ In 2013, Tomas Mikolov at published [two](https://arxiv.org/pdf/1310.4546.pdf) [papers](https://arxiv.org/pdf/1301.3781.pdf) that shook the NLP community\n",
    "+ Using a simple neural network, he demonstrated impressive results in the structure of learned word embeddings\n",
    "+ We will be implementing a naive version of the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Hyperparameters for the word2vec model\n",
    "    \"\"\"\n",
    "    # embedding dimension\n",
    "    emb_dim = 50\n",
    "\n",
    "    # Training options.\n",
    "    # The training text file.\n",
    "    train_data = \"alice-in-wonderland.txt\"\n",
    "\n",
    "    # batch size for training\n",
    "    batch_size = 64\n",
    "\n",
    "    # Number of epochs to train. After these many epochs, the learning\n",
    "    # rate decays linearly to zero and the training stops.\n",
    "    epochs_to_train = 4\n",
    "\n",
    "    # The number of words to predict to the left and right of the target word.\n",
    "    window_size = 5\n",
    "\n",
    "    # The minimum number of word occurrences for it to be included in the\n",
    "    # vocabulary.\n",
    "    min_count = 5\n",
    "\n",
    "    # Upper limit on the size of our vocabulary\n",
    "    vocab_size = 2500\n",
    "    \n",
    "    # train test split\n",
    "    train_test_split = 0.8\n",
    "    \n",
    "    # interval for printing loss statistics\n",
    "    print_interval = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "+ To construct our dataset, we will simply tokenize the entire text of the book we are interested in\n",
    "+ Then we will form the pairs of words within our window \n",
    "+ Finally, we'll shuffle and batch our training data and pack it into an iterator for the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array, dictionary, _, _ = dataloader.build_word_array(Config.train_data, vocab_size=Config.vocab_size)\n",
    "int2word = {dictionary[k]: k for k in dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairs(words):\n",
    "    \"\"\"Builds pairs of center words and context words\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def batch_and_shuffle(dataset, batch_size):\n",
    "    \"\"\"Returns an iterator of the shuffle and batched dataset\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2500, Dataset size: 320938\n"
     ]
    }
   ],
   "source": [
    "dataset = build_pairs(word_array)\n",
    "print(f\"Vocab size: {len(dictionary)}, Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1.x.x\n",
    "\n",
    "+ Numerical computation library that supports automatic-differentiation -> perfect for deep learning!\n",
    "+ Often difficult and annoying to use since it's almost a compiled language inside of an interpreted language (Python)\n",
    "+ The user defines \"nodes\" in the tensorflow graph\n",
    "+ The user must then compile the graph in a tensorflow session, at which point data can be fed into the nodes and computation can execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside session\n",
      "x:  Tensor(\"Placeholder:0\", shape=(), dtype=int32)\n",
      "y:  Tensor(\"Const:0\", shape=(), dtype=int32)\n",
      "z:  Tensor(\"Const_1:0\", shape=(), dtype=int32)\n",
      "s:  Tensor(\"add_1:0\", shape=(), dtype=int32)\n",
      "Inside session\n",
      "x:  Error! We have to feed a value into a placeholder inside a session!\n",
      "y:  9\n",
      "z:  10\n",
      "s:  20\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32, shape=[])\n",
    "y = tf.constant(9)\n",
    "z = tf.constant(10)\n",
    "\n",
    "# We never defined x, but this doesn't throw an error!\n",
    "s = x + y + z\n",
    "\n",
    "# Prints the signature of the tensors and not the data\n",
    "print(\"Outside session\")\n",
    "print(\"x: \", x)\n",
    "print(\"y: \", y)\n",
    "print(\"z: \", z)\n",
    "print(\"s: \", s)\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(\"Inside session\")\n",
    "    try:\n",
    "        print(\"x: \", session.run(x))\n",
    "    except:\n",
    "        print(\"x: \", \"Error! We have to feed a value into a placeholder inside a session!\")\n",
    "    print(\"y: \", session.run(y))\n",
    "    print(\"z: \", session.run(z))\n",
    "    print(\"s: \", session.run(s, feed_dict={x: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x_train = tf.placeholder(tf.int32, shape=(None,))\n",
    "y_label = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# one-hot encode the input vector\n",
    "x_onehot = tf.one_hot(x_train, Config.vocab_size, dtype=tf.float32)\n",
    "# convert the labels to one hot vectors\n",
    "y_onehot = tf.one_hot(y_label, Config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "# forward pass\n",
    "W1 = tf.Variable(tf.random_normal([Config.vocab_size, Config.emb_dim]))\n",
    "b1 = tf.Variable(tf.random_normal([Config.emb_dim])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x_onehot, W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([Config.emb_dim, Config.vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([Config.vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 1, Iteration:    0, Loss: 24.6571\n",
      "Epoch: 1, Iteration: 1000, Loss: 10.4855\n",
      "Epoch: 1, Iteration: 2000, Loss: 9.7891\n",
      "Epoch: 1, Iteration: 3000, Loss: 7.5740\n",
      "Epoch: 1, Iteration: 4000, Loss: 8.0578\n",
      "Epoch: 1, Iteration: 5000, Loss: 7.1415\n",
      "Epoch: 2, Iteration:    0, Loss: 6.3902\n",
      "Epoch: 2, Iteration: 1000, Loss: 6.5592\n",
      "Epoch: 2, Iteration: 2000, Loss: 6.0102\n",
      "Epoch: 2, Iteration: 3000, Loss: 6.8530\n",
      "Epoch: 2, Iteration: 4000, Loss: 5.9844\n",
      "Epoch: 2, Iteration: 5000, Loss: 6.4972\n",
      "Epoch: 3, Iteration:    0, Loss: 6.5580\n",
      "Epoch: 3, Iteration: 1000, Loss: 5.8630\n",
      "Epoch: 3, Iteration: 2000, Loss: 6.2673\n",
      "Epoch: 3, Iteration: 3000, Loss: 6.3334\n",
      "Epoch: 3, Iteration: 4000, Loss: 6.2530\n",
      "Epoch: 3, Iteration: 5000, Loss: 6.2560\n",
      "Epoch: 4, Iteration:    0, Loss: 5.5145\n",
      "Epoch: 4, Iteration: 1000, Loss: 5.8168\n",
      "Epoch: 4, Iteration: 2000, Loss: 5.4571\n",
      "Epoch: 4, Iteration: 3000, Loss: 5.8524\n",
      "Epoch: 4, Iteration: 4000, Loss: 5.6202\n",
      "Epoch: 4, Iteration: 5000, Loss: 6.2988\n"
     ]
    }
   ],
   "source": [
    "# define the loss function\n",
    "xent_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.8).minimize(xent_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initialize all global variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(Config.epochs_to_train):\n",
    "        \n",
    "        for i, (x, y) in enumerate(batch_and_shuffle(dataset, Config.batch_size)):\n",
    "            _, loss = sess.run([train_step, xent_loss], feed_dict={x_train: x, y_label: y})\n",
    "            \n",
    "            if i % Config.print_interval == 0:\n",
    "                print(f\"Epoch: {epoch + 1}, Iteration: {i:4d}, Loss: {loss:.4f}\")\n",
    "                \n",
    "    vectors = sess.run((W1 + tf.transpose(W2)) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization\n",
    "+ Once we have trained our model, we can pull the weights from the network and visualize them\n",
    "+ We will use the Tensorflow [Embedding Projector](https://projector.tensorflow.org) to visualize our learned embeddings\n",
    "+ First we need to write the necessary data to files for the Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embeddings(vectors, filename=f\"word2vec_{Config.emb_dim}d.tsv\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, vector in enumerate(vectors):\n",
    "            # convert the embedding to string\n",
    "            write_str = '\\t'.join([int2word[i]] + [str(v) for v in vector]) \n",
    "            if i < vectors.shape[0] - 1:\n",
    "                write_str += '\\n'\n",
    "                                  \n",
    "            f.write(write_str)\n",
    "    print(f\"Wrote embeddings to {filename}\")\n",
    "    \n",
    "def write_labels(int2word, filename=f\"label_metadata.tsv\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in int2word:\n",
    "            write_str = int2word[i]\n",
    "            if i < len(int2word) - 1:\n",
    "                write_str += \"\\n\"\n",
    "            f.write(write_str)\n",
    "    print(f\"Wrote labels to {filename}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote embeddings to word2vec_50d.tsv\n",
      "Wrote labels to label_metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "write_embeddings(vectors)\n",
    "write_labels(int2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
