# Word Embeddings
A simple introduction to word embeddings and Tomas Mikolov's [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). What we'll cover:

1. The motivation for continuous representations of words
2. The skip-gram model for predicting context words
3. TensorFlow basics (pre-version 2.0)
4. TensorFlow's [Embedding Projector](https://projector.tensorflow.org/) for visualising learned embeddings

### Warning
+ Make sure the `wget` command is installed on your machine. If you are on Mac and have Homebrew, you can run `brew install wget` 
+ This notebook lacks detailed markdown/LaTeX explainations, but will be updated accordingly. For now, please refer to the links provided in the notebook for more details.
