{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Disclaimer: Material is credited to\n",
    "+ [Stanford 224N's](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf) lecture on word embeddings\n",
    "\n",
    "\n",
    "+ [Pat Coady's](https://github.com/pat-coady/word2vec) repo\n",
    "\n",
    "\n",
    "\n",
    "+ [This](https://gist.github.com/aneesh-joshi/c8a451502958fa367d84bf038081ee4b) amazing github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representations\n",
    "\n",
    "## How can we turn words into objects that a computer can process?\n",
    "\n",
    "## Words as discrete representations:\n",
    "+ In traditional NLP, words are often represented as unique one-hot encoded vectors, e.g. cat = [0, 0, 0, 1], dog = [0, 0, 1, 0]\n",
    "+ The length of these vectors is determined by the vocabulary size of the training data, i.e. how many unqiue words there are\n",
    "+ The problems with this approach?\n",
    "    - A lot of memory required to represent a large corpus, Need [V x V] matrix where V is the vocabulary size\n",
    "    - These word vectors are sparse, most of the elements in the word vector matrix are 0\n",
    "    - There is no natural measure of similarity between these word vectors,\n",
    "        i.e. hotel = [0, 0, 1], motel = [0, 1, 0] are orthogonal vectors with similar meaning\n",
    "    - **We want to encode similarity within the word vectors**\n",
    "        \n",
    "## Words as continuous representations\n",
    "+ Distributional semantics is the notion that a word's meaning is given by the words that appear close by\n",
    "+ “You shall know a word by the company it keeps” - J. R. Firth 1957\n",
    "+ One of the most successful ideas of modern statistical NLP!\n",
    "+ We will use dense representations of words so that it is similar to words that often appear in the same context\n",
    "+ Example: cat = [0.3, 1.5, -0.4, 1.9]\n",
    "+ With continuous representations, we can naturally measure similarity by using the dot product in the vector space formed by our vectors!\n",
    "+ The length of the vectors in this case is a hyperparameter, so we only need a [V x D] matrix with D < V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data.dataloader as dataloader\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# for deterministic results\n",
    "tf.random.set_random_seed(1021)\n",
    "\n",
    "assert (tf.__version__ == \"1.13.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download our data\n",
    "!curl https://www.gutenberg.org/files/11/11-0.txt -o data/alice-in-wonderland.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "+ In 2013, Tomas Mikolov at published [two](https://arxiv.org/pdf/1310.4546.pdf) [papers](https://arxiv.org/pdf/1301.3781.pdf) that shook the NLP community\n",
    "+ Using a simple neural network, he demonstrated impressive results in the structure of learned word embeddings\n",
    "+ We will be implementing a naive version of the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Hyperparameters for the word2vec model\n",
    "    \"\"\"\n",
    "    # embedding dimension\n",
    "    emb_dim = 50\n",
    "\n",
    "    # Training options.\n",
    "    # The training text file.\n",
    "    train_data = \"data/alice-in-wonderland.txt\"\n",
    "\n",
    "    # batch size for training\n",
    "    batch_size = 64\n",
    "\n",
    "    # Number of epochs to train. After these many epochs, the learning\n",
    "    # rate decays linearly to zero and the training stops.\n",
    "    epochs_to_train = 4\n",
    "\n",
    "    # The number of words to predict to the left and right of the target word.\n",
    "    window_size = 5\n",
    "\n",
    "    # The minimum number of word occurrences for it to be included in the\n",
    "    # vocabulary.\n",
    "    min_count = 5\n",
    "\n",
    "    # Upper limit on the size of our vocabulary\n",
    "    vocab_size = 2500\n",
    "    \n",
    "    # train test split\n",
    "    train_test_split = 0.8\n",
    "    \n",
    "    # interval for printing loss statistics\n",
    "    print_interval = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "+ To construct our dataset, we will simply tokenize the entire text of the book we are interested in\n",
    "+ Then we will form the pairs of words within our window \n",
    "+ Finally, we'll shuffle and batch our training data and pack it into an iterator for the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array, dictionary, _, _ = dataloader.build_word_array(Config.train_data, vocab_size=Config.vocab_size)\n",
    "int2word = {dictionary[k]: k for k in dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairs(words):\n",
    "    \"\"\"Builds pairs of center words and context words\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def batch_and_shuffle(dataset, batch_size):\n",
    "    \"\"\"Returns an iterator of the shuffle and batched dataset\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_pairs(word_array)\n",
    "print(f\"Vocab size: {len(dictionary)}, Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 1.x.x\n",
    "\n",
    "+ Numerical computation library that supports automatic-differentiation -> perfect for deep learning!\n",
    "+ Often difficult and annoying to use since it's almost a compiled language inside of an interpreted language (Python)\n",
    "+ The user defines \"nodes\" in the tensorflow graph\n",
    "+ The user must then compile the graph in a tensorflow session, at which point data can be fed into the nodes and computation can execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, shape=[])\n",
    "y = tf.constant(9)\n",
    "z = tf.constant(10)\n",
    "\n",
    "# We never defined x, but this doesn't throw an error!\n",
    "s = x + y + z\n",
    "\n",
    "# Prints the signature of the tensors and not the data\n",
    "print(\"Outside session\")\n",
    "print(\"x: \", x)\n",
    "print(\"y: \", y)\n",
    "print(\"z: \", z)\n",
    "print(\"s: \", s)\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(\"Inside session\")\n",
    "    try:\n",
    "        print(\"x: \", session.run(x))\n",
    "    except:\n",
    "        print(\"x: \", \"Error! We have to feed a value into a placeholder inside a session!\")\n",
    "    print(\"y: \", session.run(y))\n",
    "    print(\"z: \", session.run(z))\n",
    "    print(\"s: \", session.run(s, feed_dict={x: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x_train = tf.placeholder(tf.int32, shape=(None,))\n",
    "y_label = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# one-hot encode the input vector\n",
    "x_onehot = tf.one_hot(x_train, Config.vocab_size, dtype=tf.float32)\n",
    "# convert the labels to one hot vectors\n",
    "y_onehot = tf.one_hot(y_label, Config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "### forward pass ###\n",
    "W1 = tf.Variable(tf.random_normal([Config.vocab_size, Config.emb_dim]))\n",
    "b1 = tf.Variable(tf.random_normal([Config.emb_dim]))\n",
    "\n",
    "# compute the hidden representation\n",
    "hidden = # affine transformation\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([Config.emb_dim, Config.vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([Config.vocab_size]))\n",
    "\n",
    "# compute the predictions\n",
    "prediction = # affine transformation, then softmax normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "\n",
    "\n",
    "# define the training step:\n",
    "\n",
    "\n",
    "# start the tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # initialize all global variables\n",
    "\n",
    "    \n",
    "    # run the training loop\n",
    "    for epoch in range(Config.epochs_to_train):\n",
    "        \n",
    "        # train on batches\n",
    "        for i, (x, y) in enumerate(batch_and_shuffle(dataset, Config.batch_size)):\n",
    "            \n",
    "            # run the tensorflow training ops\n",
    "\n",
    "            \n",
    "            # log the progress\n",
    "            if i % Config.print_interval == 0:\n",
    "                print(f\"Epoch: {epoch + 1}, Iteration: {i:4d}, Loss: {loss:.4f}\")\n",
    "                \n",
    "    # average the two embedding matrices\n",
    "    vectors = sess.run( \"\"\" what goes here? \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization\n",
    "+ Once we have trained our model, we can pull the weights from the network and visualize them\n",
    "+ We will use the Tensorflow [Embedding Projector](https://projector.tensorflow.org) to visualize our learned embeddings\n",
    "+ We need to write the embedding vectors and our vocab into tsv files for the Embedding Projector to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_embeddings(vectors, filename=f\"data/word2vec_{Config.emb_dim}d.tsv\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, vector in enumerate(vectors):\n",
    "            # convert the embedding to string\n",
    "            write_str = '\\t'.join([int2word[i]] + [str(v) for v in vector]) \n",
    "            if i < vectors.shape[0] - 1:\n",
    "                write_str += '\\n'\n",
    "                                  \n",
    "            f.write(write_str)\n",
    "    print(f\"Wrote embeddings to {filename}\")\n",
    "    \n",
    "def write_labels(int2word, filename=f\"data/label_metadata.tsv\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in int2word:\n",
    "            write_str = int2word[i]\n",
    "            if i < len(int2word) - 1:\n",
    "                write_str += \"\\n\"\n",
    "            f.write(write_str)\n",
    "    print(f\"Wrote labels to {filename}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_embeddings(vectors)\n",
    "write_labels(int2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
